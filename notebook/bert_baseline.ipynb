{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import cuda\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "from config import settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1' if cuda.is_available() else 'cpu'\n",
    "\n",
    "MAX_LEN = 150\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "DISTIL_BERT_CHECKPOINT = 'distilbert-base-uncased'\n",
    "RUN_NAME = 'ROS'\n",
    "TEST_PATH = '../data/processed/quick_test.csv'\n",
    "TRAIN_PATH = '../data/ros/train.csv'\n",
    "MODEL_SAVE = '../models/'\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(DISTIL_BERT_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuoraDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_path, tokenizer, max_len):\n",
    "        self._dataset = pd.read_csv(file_path, low_memory=False)\n",
    "        self._tokenizer = tokenizer \n",
    "        self._max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self._dataset.iloc[index][\"question_text\"]\n",
    "        inputs = self._tokenizer(\n",
    "            [text],\n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self._max_len,\n",
    "            padding='max_length'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"ids\": inputs[\"input_ids\"],\n",
    "            \"mask\": inputs[\"attention_mask\"],\n",
    "            \"target\": torch.tensor(self._dataset.iloc[index][\"target\"], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QuoraDataset(TRAIN_PATH, tokenizer, MAX_LEN)\n",
    "test_dataset = QuoraDataset(TEST_PATH, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilBert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertModelClass(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistilBertModelClass, self).__init__()\n",
    "        self.distil_bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.linear1 = nn.Linear(768, 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, ids, mask):\n",
    "        bert_out = self.distil_bert(ids, mask)\n",
    "        x = bert_out.last_hidden_state[:, -1, :] # get bert last hidden state\n",
    "        x = self.linear1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = DistilBertModelClass()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "\n",
    "def accuracy(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "\n",
    "        classname = {0: 'Sincere', 1: 'Insincere'}\n",
    "        correct_pred = defaultdict(lambda: 0)\n",
    "        total_pred = defaultdict(lambda: 0)\n",
    "\n",
    "        for inputs in loader:\n",
    "            ids = inputs['ids'].squeeze(1).to(device)\n",
    "            mask = inputs['mask'].squeeze(1).to(device)\n",
    "            targets = inputs['target'].to(device)\n",
    "\n",
    "            output = model(ids, mask).squeeze()\n",
    "\n",
    "            _, predictions = torch.max(output, 1)\n",
    "            \n",
    "            y_pred += list(predictions.to('cpu'))\n",
    "            y_true += list(targets.to('cpu'))\n",
    "\n",
    "            for target, prediction in zip(targets, predictions):\n",
    "                if target.item() == prediction.item():\n",
    "                    correct_pred[classname[target.item()]] += 1\n",
    "                total_pred[classname[prediction.item()]] += 1\n",
    "\n",
    "        results = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'f1': f1_score(y_true, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_true, y_pred)\n",
    "        }\n",
    "\n",
    "        for classname, correct_count in correct_pred.items():\n",
    "            results['precision_' + classname] = 100 * float(correct_count) / total_pred[classname]\n",
    "\n",
    "        return results\n",
    "\n",
    "results = accuracy(model, test_loader)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch=1):\n",
    "    model.train()\n",
    "\n",
    "    for idx, inputs in enumerate(train_loader):\n",
    "        \n",
    "        ids = inputs['ids'].squeeze(1).to(device)\n",
    "        mask = inputs['mask'].squeeze(1).to(device)\n",
    "        target = inputs['target'].to(device)\n",
    "\n",
    "        output = model(ids, mask).squeeze()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        l = loss(output, target)\n",
    "        l.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log Loss\n",
    "        run[\"train/loss\"].log(l.item())\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            print(f'Epoch: {epoch}, {idx}/{len(train_loader)}, Loss:  {l.item()}')\n",
    "\n",
    "        if idx % 1_000 == 0:\n",
    "            results = accuracy(model, test_loader) \n",
    "            run[\"train/accuracy\"] = results['accuracy']\n",
    "            run[\"train/f1\"] = results['f1']\n",
    "            run[\"train/roc_auc\"] = results['roc_auc']\n",
    "            run[\"train/precision_Sincere\"] = results['precision_Sincere']\n",
    "            run[\"train/precision_Insincere\"] = results['precision_Insincere']\n",
    "            print(results)\n",
    "            print(\"Saving model...\")\n",
    "            torch.save(model.state_dict(), Path(MODEL_SAVE) / f'ftbert_{idx}_{datetime.now()}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track training and results...\n",
    "import neptune.new as neptune\n",
    "\n",
    "run = neptune.init(\n",
    "    project=settings.project,\n",
    "    api_token=settings.api_token,\n",
    "    name='RandomOversampling'\n",
    ")  \n",
    "\n",
    "train(epoch=EPOCHS)\n",
    "\n",
    "run.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e9582e8547f3228029591ab69722d0e7c67d7caf2f49c6003d3efa221005ccd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
