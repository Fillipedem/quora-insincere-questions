{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import cuda\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "from config import settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1' if cuda.is_available() else 'cpu'\n",
    "\n",
    "MAX_LEN = 150\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "DISTIL_BERT_CHECKPOINT = 'distilbert-base-uncased'\n",
    "RUN_NAME = 'ROS'\n",
    "TEST_PATH = '../data/processed/quick_test.csv'\n",
    "TRAIN_PATH = '../data/ros/train.csv'\n",
    "MODEL_SAVE = '../models/'\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(DISTIL_BERT_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuoraDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_path, tokenizer, max_len):\n",
    "        self._dataset = pd.read_csv(file_path, low_memory=False)\n",
    "        self._tokenizer = tokenizer \n",
    "        self._max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self._dataset.iloc[index][\"question_text\"]\n",
    "        inputs = self._tokenizer(\n",
    "            [text],\n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self._max_len,\n",
    "            padding='max_length'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"ids\": inputs[\"input_ids\"],\n",
    "            \"mask\": inputs[\"attention_mask\"],\n",
    "            \"target\": torch.tensor(self._dataset.iloc[index][\"target\"], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QuoraDataset(TRAIN_PATH, tokenizer, MAX_LEN)\n",
    "test_dataset = QuoraDataset(TEST_PATH, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilBert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class DistilBertModelClass(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistilBertModelClass, self).__init__()\n",
    "        self.distil_bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.linear1 = nn.Linear(768, 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, ids, mask):\n",
    "        bert_out = self.distil_bert(ids, mask)\n",
    "        x = bert_out.last_hidden_state[:, -1, :] # get bert last hidden state\n",
    "        x = self.linear1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = DistilBertModelClass()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.20977393617021275,\n",
       " 'f1': 0.26613152207471447,\n",
       " 'roc_auc': 0.5047354954236274,\n",
       " 'precision_Insincere': 15.52593659942363,\n",
       " 'precision_Sincere': 86.20689655172414}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from collections import defaultdict\n",
    "\n",
    "def accuracy(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "\n",
    "        classname = {0: 'Sincere', 1: 'Insincere'}\n",
    "        correct_pred = defaultdict(lambda: 0)\n",
    "        total_pred = defaultdict(lambda: 0)\n",
    "\n",
    "        for inputs in loader:\n",
    "            ids = inputs['ids'].squeeze(1).to(device)\n",
    "            mask = inputs['mask'].squeeze(1).to(device)\n",
    "            targets = inputs['target'].to(device)\n",
    "\n",
    "            output = model(ids, mask).squeeze()\n",
    "\n",
    "            _, predictions = torch.max(output, 1)\n",
    "            \n",
    "            y_pred += list(predictions.to('cpu'))\n",
    "            y_true += list(targets.to('cpu'))\n",
    "\n",
    "            for target, prediction in zip(targets, predictions):\n",
    "                if target.item() == prediction.item():\n",
    "                    correct_pred[classname[target.item()]] += 1\n",
    "                total_pred[classname[prediction.item()]] += 1\n",
    "\n",
    "        results = {\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'f1': f1_score(y_true, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_true, y_pred)\n",
    "        }\n",
    "\n",
    "        for classname, correct_count in correct_pred.items():\n",
    "            results['precision_' + classname] = 100 * float(correct_count) / total_pred[classname]\n",
    "\n",
    "        return results\n",
    "\n",
    "results = accuracy(model, test_loader)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch=1):\n",
    "    model.train()\n",
    "\n",
    "    for idx, inputs in enumerate(train_loader):\n",
    "        \n",
    "        ids = inputs['ids'].squeeze(1).to(device)\n",
    "        mask = inputs['mask'].squeeze(1).to(device)\n",
    "        target = inputs['target'].to(device)\n",
    "\n",
    "        output = model(ids, mask).squeeze()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        l = loss(output, target)\n",
    "        l.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log Loss\n",
    "        run[\"train/loss\"].log(l.item())\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            print(f'Epoch: {epoch}, {idx}/{len(train_loader)}, Loss:  {l.item()}')\n",
    "\n",
    "        if idx % 20 == 0:\n",
    "            results = accuracy(model, test_loader) \n",
    "            run[\"train/accuracy\"] = results['accuracy']\n",
    "            run[\"train/f1\"] = results['f1']\n",
    "            run[\"train/roc_auc\"] = results['roc_auc']\n",
    "            run[\"train/precision_Sincere\"] = results['precision_Sincere']\n",
    "            run[\"train/precision_Insincere\"] = results['precision_Insincere']\n",
    "            print(results)\n",
    "            print(\"Saving model...\")\n",
    "            torch.save(model.state_dict(), Path(MODEL_SAVE) / f'ftbert_{idx}_{datetime.now()}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/demenezes/Mestrado-RI/e/MES-6\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n",
      "Epoch: 1, 0/13497, Loss:  0.6846345067024231\n",
      "{'accuracy': 0.1761968085106383, 'f1': 0.267297457125961, 'roc_auc': 0.5034451153534436, 'precision_Insincere': 15.484755053100377, 'precision_Sincere': 87.64044943820225}\n",
      "Saving model...\n",
      "Epoch: 1, 10/13497, Loss:  0.6750589609146118\n",
      "Epoch: 1, 20/13497, Loss:  0.6509659886360168\n",
      "Epoch: 1, 30/13497, Loss:  0.6095486879348755\n",
      "Epoch: 1, 40/13497, Loss:  0.5514026880264282\n",
      "Epoch: 1, 50/13497, Loss:  0.49052292108535767\n",
      "Epoch: 1, 60/13497, Loss:  0.476421594619751\n",
      "Epoch: 1, 70/13497, Loss:  0.4465118944644928\n",
      "Epoch: 1, 80/13497, Loss:  0.4685976207256317\n",
      "Epoch: 1, 90/13497, Loss:  0.42306268215179443\n",
      "Epoch: 1, 100/13497, Loss:  0.456206351518631\n",
      "Epoch: 1, 110/13497, Loss:  0.4823126196861267\n",
      "Epoch: 1, 120/13497, Loss:  0.4374268352985382\n",
      "Epoch: 1, 130/13497, Loss:  0.43227869272232056\n",
      "Epoch: 1, 140/13497, Loss:  0.40552234649658203\n",
      "Epoch: 1, 150/13497, Loss:  0.4238656163215637\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8da8dd7dc535>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m )  \n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f756fbfc1ae3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# track training and results...\n",
    "import neptune.new as neptune\n",
    "\n",
    "run = neptune.init(\n",
    "    project=settings.project,\n",
    "    api_token=settings.api_token,\n",
    "    name='RandomOversampling'\n",
    ")  \n",
    "\n",
    "train(epoch=EPOCHS)\n",
    "\n",
    "run.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e9582e8547f3228029591ab69722d0e7c67d7caf2f49c6003d3efa221005ccd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
