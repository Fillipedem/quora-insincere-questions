{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import cuda\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "from config import settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1' if cuda.is_available() else 'cpu'\n",
    "\n",
    "MAX_LEN = 150\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "DISTIL_BERT_CHECKPOINT = 'distilbert-base-uncased'\n",
    "RUN_NAME = 'ROS'\n",
    "TEST_PATH = '../data/processed/test.csv'\n",
    "TRAIN_PATH = '../data/ros/train.csv'\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(DISTIL_BERT_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuoraDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_path, tokenizer, max_len):\n",
    "        self._dataset = pd.read_csv(file_path, low_memory=False)\n",
    "        self._tokenizer = tokenizer \n",
    "        self._max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self._dataset.iloc[index][\"question_text\"]\n",
    "        inputs = self._tokenizer(\n",
    "            [text],\n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self._max_len,\n",
    "            padding='max_length'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"ids\": inputs[\"input_ids\"],\n",
    "            \"mask\": inputs[\"attention_mask\"],\n",
    "            \"target\": torch.tensor(self._dataset.iloc[index][\"target\"], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QuoraDataset(TRAIN_PATH, tokenizer, MAX_LEN)\n",
    "test_dataset = QuoraDataset(TEST_PATH, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilBert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class DistilBertModelClass(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistilBertModelClass, self).__init__()\n",
    "        self.distil_bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.linear1 = nn.Linear(768, 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, ids, mask):\n",
    "        bert_out = self.distil_bert(ids, mask)\n",
    "        x = bert_out.last_hidden_state[:, -1, :] # get bert last hidden state\n",
    "        x = self.linear1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = DistilBertModelClass()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def accuracy(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        classname = {0: 'Sincere', 1: 'Insincere'}\n",
    "        correct_pred = defaultdict(lambda: 0)\n",
    "        total_pred = defaultdict(lambda: 0)\n",
    "\n",
    "        for idx, inputs in enumerate(loader):\n",
    "            ids = inputs['ids'].squeeze(1).to(device)\n",
    "            mask = inputs['mask'].squeeze(1).to(device)\n",
    "            targets = inputs['target'].to(device)\n",
    "\n",
    "            output = model(ids, mask).squeeze()\n",
    "\n",
    "            _, predictions = torch.max(output, 1)\n",
    "            \n",
    "            for target, prediction in zip(targets, predictions):\n",
    "                if target.item() == prediction.item():\n",
    "                    correct_pred[classname[target.item()]] += 1\n",
    "                total_pred[classname[target.item()]] += 1\n",
    "\n",
    "        class_acc = {}\n",
    "        for classname, correct_count in correct_pred.items():\n",
    "            class_acc[classname] = 100 * float(correct_count) / total_pred[classname]\n",
    "        \n",
    "        return correct_pred, class_acc\n",
    "        \n",
    "accuracy(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch=1):\n",
    "    model.train()\n",
    "\n",
    "    for idx, inputs in enumerate(train_loader):\n",
    "        \n",
    "        ids = inputs['ids'].squeeze(1).to(device)\n",
    "        mask = inputs['mask'].squeeze(1).to(device)\n",
    "        target = inputs['target'].to(device)\n",
    "\n",
    "        output = model(ids, mask).squeeze()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        l = loss(output, target)\n",
    "        l.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            print(f'Epoch: {epoch}, {idx}/{len(train_loader)}, Loss:  {l.item()}')\n",
    "\n",
    "        if idx % 1_000 == 0 and idx > 1:\n",
    "            correct_pred, class_acc = accuracy(model, test_loader)\n",
    "            print(f'Validation Accuracy: {class_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track training and results...\n",
    "import neptune.new as neptune\n",
    "\n",
    "run = neptune.init(\n",
    "    project=settings.project,\n",
    "    api_token=settings.api_token\n",
    ")  \n",
    "\n",
    "train(epoch=EPOCHS)\n",
    "\n",
    "run.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e9582e8547f3228029591ab69722d0e7c67d7caf2f49c6003d3efa221005ccd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
