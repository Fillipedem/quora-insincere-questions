{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import cuda\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1' if cuda.is_available() else 'cpu'\n",
    "\n",
    "MAX_LEN = 150\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "DISTIL_BERT_CHECKPOINT = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuoraDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_path, tokenizer, max_len):\n",
    "        self._dataset = pd.read_csv(file_path, low_memory=False)\n",
    "        self._tokenizer = tokenizer \n",
    "        self._max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self._dataset.iloc[index][\"question_text\"]\n",
    "        inputs = self._tokenizer(\n",
    "            [text],\n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self._max_len,\n",
    "            padding='max_length'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"ids\": inputs[\"input_ids\"],\n",
    "            \"mask\": inputs[\"attention_mask\"],\n",
    "            \"target\": torch.tensor(self._dataset.iloc[index][\"target\"], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QuoraDataset(\"../data/processed/train_resampled.csv\", tokenizer, MAX_LEN)\n",
    "test_dataset = QuoraDataset(\"../data/processed/test.csv\", tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilBert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class DistilBertModelClass(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistilBertModelClass, self).__init__()\n",
    "        self.distil_bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.linear1 = nn.Linear(768, 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, ids, mask):\n",
    "        bert_out = self.distil_bert(ids, mask)\n",
    "        x = bert_out.last_hidden_state[:, -1, :] # get bert last hidden state\n",
    "        x = self.linear1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "model = DistilBertModelClass()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: Sincere is 25.9 %\n",
      "Accuracy for class: Insincere is 73.9 %\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def accuracy(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        classname = {0: 'Sincere', 1: 'Insincere'}\n",
    "        correct_pred = defaultdict(lambda: 0)\n",
    "        total_pred = defaultdict(lambda: 0)\n",
    "\n",
    "        for idx, inputs in enumerate(loader):\n",
    "            ids = inputs['ids'].squeeze(1).to(device)\n",
    "            mask = inputs['mask'].squeeze(1).to(device)\n",
    "            targets = inputs['target'].to(device)\n",
    "\n",
    "            output = model(ids, mask).squeeze()\n",
    "\n",
    "            _, predictions = torch.max(output, 1)\n",
    "            \n",
    "            for target, prediction in zip(targets, predictions):\n",
    "                if target.item() == prediction.item():\n",
    "                    correct_pred[classname[target.item()]] += 1\n",
    "                total_pred[classname[target.item()]] += 1\n",
    "\n",
    "            # Remove after\n",
    "            if idx > 100:\n",
    "                break\n",
    "\n",
    "        for classname, correct_count in correct_pred.items():\n",
    "            accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "            print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
    "\n",
    "accuracy(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch=1):\n",
    "    model.train()\n",
    "\n",
    "    for idx, inputs in enumerate(train_loader):\n",
    "        \n",
    "        ids = inputs['ids'].squeeze(1).to(device)\n",
    "        mask = inputs['mask'].squeeze(1).to(device)\n",
    "        target = inputs['target'].to(device)\n",
    "\n",
    "        output = model(ids, mask).squeeze()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        l = loss(output, target)\n",
    "        l.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            print(f'Epoch: {epoch}, {idx}/{len(train_loader)}, Loss:  {l.item()}')\n",
    "\n",
    "        if idx % 1_000 == 0 and idx > 1:\n",
    "            print(f'Validation Accuracy: {accuracy(model, test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, 0/25643, Loss:  0.6937118172645569\n",
      "Epoch: 1, 0/25643, Loss:  0.6937118172645569\n",
      "Epoch: 1, 1/25643, Loss:  0.6957258582115173\n",
      "Epoch: 1, 2/25643, Loss:  0.692252516746521\n",
      "Epoch: 1, 3/25643, Loss:  0.6909288167953491\n",
      "Epoch: 1, 4/25643, Loss:  0.6889779567718506\n",
      "Epoch: 1, 5/25643, Loss:  0.6867613792419434\n",
      "Epoch: 1, 6/25643, Loss:  0.6896056532859802\n",
      "Epoch: 1, 7/25643, Loss:  0.6796237826347351\n",
      "Epoch: 1, 8/25643, Loss:  0.6830148696899414\n",
      "Epoch: 1, 9/25643, Loss:  0.6773112416267395\n",
      "Epoch: 1, 10/25643, Loss:  0.6825545430183411\n",
      "Epoch: 1, 10/25643, Loss:  0.6825545430183411\n",
      "Epoch: 1, 11/25643, Loss:  0.6748477816581726\n",
      "Epoch: 1, 12/25643, Loss:  0.6784551739692688\n",
      "Epoch: 1, 13/25643, Loss:  0.6669279932975769\n",
      "Epoch: 1, 14/25643, Loss:  0.6794577836990356\n",
      "Epoch: 1, 15/25643, Loss:  0.6706261038780212\n",
      "Epoch: 1, 16/25643, Loss:  0.6712438464164734\n"
     ]
    }
   ],
   "source": [
    "train(epoch=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e9582e8547f3228029591ab69722d0e7c67d7caf2f49c6003d3efa221005ccd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
