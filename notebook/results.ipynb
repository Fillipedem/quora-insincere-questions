{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "* Check and compare results from the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import cuda\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "from config import settings\n",
    "\n",
    "device = 'cuda:1' if cuda.is_available() else 'cpu'\n",
    "\n",
    "MAX_LEN = 150\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "DISTIL_BERT_CHECKPOINT = 'distilbert-base-uncased'\n",
    "TEST_PATH = '../data/processed/test.csv'\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(DISTIL_BERT_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuoraDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_path, tokenizer, max_len):\n",
    "        self._dataset = pd.read_csv(file_path, low_memory=False)\n",
    "        self._tokenizer = tokenizer \n",
    "        self._max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self._dataset.iloc[index][\"question_text\"]\n",
    "        inputs = self._tokenizer(\n",
    "            [text],\n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self._max_len,\n",
    "            padding='max_length'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"ids\": inputs[\"input_ids\"],\n",
    "            \"mask\": inputs[\"attention_mask\"],\n",
    "            \"target\": torch.tensor(self._dataset.iloc[index][\"target\"], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dataset)\n",
    "\n",
    "\n",
    "test_dataset = QuoraDataset(TEST_PATH, tokenizer, MAX_LEN)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class DistilBertModelClass(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DistilBertModelClass, self).__init__()\n",
    "        self.distil_bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.linear1 = nn.Linear(768, 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, ids, mask):\n",
    "        bert_out = self.distil_bert(ids, mask)\n",
    "        x = bert_out.last_hidden_state[:, -1, :] # get bert last hidden state\n",
    "        x = self.linear1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "path_ros = \"../models/ros/ftbert_6500_2022-02-15 01:45:25.338435\"\n",
    "path_nlpaug = \"../models/nlpaug/ftbert_6500_2022-02-16 00:52:05.772874\"\n",
    "\n",
    "model_ros = DistilBertModelClass()\n",
    "model_ros.load_state_dict(torch.load(path_ros), strict=False)\n",
    "model_ros.to(device);\n",
    "\n",
    "model_aug = DistilBertModelClass()\n",
    "model_aug.load_state_dict(torch.load(path_nlpaug), strict=False)\n",
    "model_aug.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.OrderedDict"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def results(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "\n",
    "        for idx, inputs in enumerate(loader):\n",
    "            if idx%1_00==0:\n",
    "                print(f\"{idx}/{len(loader)}\")\n",
    "            \n",
    "            ids = inputs['ids'].squeeze(1).to(device)\n",
    "            mask = inputs['mask'].squeeze(1).to(device)\n",
    "            targets = inputs['target'].to(device)\n",
    "\n",
    "            output = model(ids, mask).squeeze()\n",
    "\n",
    "            _, predictions = torch.max(output, 1)\n",
    "            \n",
    "            y_pred += list(predictions.to('cpu'))\n",
    "            y_true += list(targets.to('cpu'))\n",
    "\n",
    "        return y_pred, y_true\n",
    "\n",
    "\n",
    "ros_results = results(model_ros, test_loader)\n",
    "nlpaug_results = results(model_aug, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertModelClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(model_ros, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e9582e8547f3228029591ab69722d0e7c67d7caf2f49c6003d3efa221005ccd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
